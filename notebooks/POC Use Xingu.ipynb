{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a854604",
   "metadata": {},
   "source": [
    "# POC Use Xingu\n",
    "\n",
    "Use this notebook to get you started with Xingu.\n",
    "\n",
    "**Do not use it here. Copy this notebook and feel free to edit, modify and experiment in there.**\n",
    "\n",
    "**Never commit your changes to the repo**, since this notebook is just a standard example and will be used by others to play with Xingu.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "Start by importing configuration bundles to train, or batch predict, or explore metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b3e96f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is a simple `.py` file in notebooks folder, full of configuration parameters for Robson\n",
    "import config_my_xingu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3224b8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup environment\n",
    "### The `env` bundle controls locations for files and databases\n",
    "#### `config_my_xingu.bundles['env']['alpha_explorer']`\n",
    "Use when working on everyday Robson improvements\n",
    "* Robson database: local SQLite\n",
    "* DVC: off\n",
    "* Query cache: on, in `../data`\n",
    "* Trained models in: `../models`\n",
    "\n",
    "#### `config_my_xingu.bundles['env']['beta_explorer']`\n",
    "Use when working with staging database\n",
    "* Robson database: staging PostgreSQL\n",
    "* DVC: on\n",
    "* Query cache: on, in `../data`\n",
    "* Trained models in: staging S3\n",
    "\n",
    "#### `config_my_xingu.bundles['env']['staging']`\n",
    "Similar to `beta_explorer`, used in GitHub staging workflow\n",
    "* Robson database: staging PostgreSQL\n",
    "* DVC: on\n",
    "* Query cache: off\n",
    "* Trained models in: staging S3\n",
    "\n",
    "#### `config_my_xingu.bundles['env']['production']`\n",
    "Do not use in your laptop, this is just documented as how to configure for production\n",
    "* Robson database: production PostgreSQL\n",
    "* DVC: on\n",
    "* Query cache: off\n",
    "* Trained models in: production S3\n",
    "\n",
    "### The `parallel` bundle controls parallelism and modus operandi\n",
    "#### `config_my_xingu.bundles['parallel']['train_and_predict']`\n",
    "Use when working on everyday Robson improvements\n",
    "* Train: yes\n",
    "    * Train parallelism: maximum\n",
    "    * Hyper-parameters optimization: use what is found in DB, or estimator default\n",
    "* Post process (pickle, metrics etc): yes\n",
    "    * Batch predict: yes\n",
    "    * Post-process parallelism: maximum\n",
    "\n",
    "#### `config_my_xingu.bundles['parallel']['predict_only']`\n",
    "Use with pre-trained models\n",
    "* Train: no\n",
    "    * Hyper-parameters optimization: no\n",
    "* Post process (pickle, metrics etc): no\n",
    "    * Post-process parallelism: maximum\n",
    "    * Batch predict: yes\n",
    "\n",
    "#### `config_my_xingu.bundles['parallel']['hyper_optimize_only']`\n",
    "Use when working on hyper-parameters optimization\n",
    "* Train: yes\n",
    "    * Train parallelism: one model at a time\n",
    "    * Hyper-parameters optimization: compute\n",
    "    * Hyper-parameters optimization parallelism: maximum\n",
    "* Post process (pickle, metrics etc): no\n",
    "    * Batch predict: no\n",
    "\n",
    "#### `config_my_xingu.bundles['parallel']['do_all']`\n",
    "Use when working on hyper-parameters optimization\n",
    "* Train: yes\n",
    "    * Train parallelism: 3 models at a time\n",
    "    * Hyper-parameters optimization: compute\n",
    "    * Hyper-parameters optimization parallelism: 6\n",
    "* Post process (pickle, metrics etc): yes\n",
    "    * Post-process parallelism: 3 models at a time\n",
    "    * Batch predict: yes\n",
    "\n",
    "Choose one **env** bundle and one **parallel** bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cbe98f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "os.environ.update(config_my_xingu.bundles['env']['alpha_explorer'])\n",
    "os.environ.update(config_my_xingu.bundles['parallel']['train_and_predict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c555e3",
   "metadata": {},
   "source": [
    "Amend anything you want to change. All values must be text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec05dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ.update(\n",
    "    dict(\n",
    "        HYPEROPT_STRATEGY     = 'dp',\n",
    "        BATCH_PREDICT         = 'false',\n",
    "        DEBUG                 = 'true'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ae362",
   "metadata": {},
   "source": [
    "## Import Xingu and configure Logging\n",
    "Next line is required if `xingu` folder not in `PYTHON_PATH` or robson not installed by pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83376138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Give priority to local packages (not needed in case Robson was installed by pip)\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(''), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e7c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import decouple\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from xingu import DataProviderFactory\n",
    "from xingu import ConfigManager\n",
    "from xingu import Coach\n",
    "from xingu import Model\n",
    "\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# Configure logging for Xingu\n",
    "logger=logging.getLogger('xingu')\n",
    "FORMATTER = logging.Formatter(\"%(asctime)s|%(levelname)s|%(name)s|%(message)s\")\n",
    "HANDLER = logging.StreamHandler()\n",
    "HANDLER.setFormatter(FORMATTER)\n",
    "logger.addHandler(HANDLER)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063214ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## POC 1. Train some Models\n",
    "A `Coach` is needed to train anything. Put more DataProvider IDs in the `data_providers` list. If you want to train models that have pre-reqs and are not training their dependencies in the same train session, pre-trained pre-req models will be efficiently loaded upfront."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38fdf3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_providers=['datarisk_cartoes']\n",
    "\n",
    "dpf=DataProviderFactory(providers_list=data_providers)\n",
    "coach=Coach(dpf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a551177b",
   "metadata": {},
   "source": [
    "!rm xingu.db* ../models/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717fcc3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "coach.team_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15693466",
   "metadata": {},
   "source": [
    "Also try `config_my_xingu.bundles['parallel']['hyper_optimize_only']` config bundle to radically change what `team_train()` does.\n",
    "\n",
    "Trained models are here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e8b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coach.trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7fe6eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Trained models can be used now to compute estimations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c030e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## POC 2. Use Pre-Trained Models for Batch Predict\n",
    "\n",
    "Reset this notebook before continuing. Run again only cells before \"POC 1\" just to setup environment.\n",
    "\n",
    "A `Coach` is needed to eficiently load pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30ebf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_providers=['datarisk_cartoes']\n",
    "dpf=DataProviderFactory(providers_list=data_providers)\n",
    "coach=Coach(dpf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022b91a3",
   "metadata": {},
   "source": [
    "Notice that `anuncios` is not in the `data_providers` list, but it will be loaded since it is a pre-req for `cartorios`, which is in the list.\n",
    "\n",
    "Pre-trained pickles will be search in and loaded from whatever is set in `TRAINED_MODELS_PATH` environment variable. This is usually set to `models` local folder or to some S3 URL.\n",
    "\n",
    "Models will be loaded in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e6d22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(os.environ['TRAINED_MODELS_PATH'])\n",
    "coach.team_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06205b32",
   "metadata": {},
   "source": [
    "Use embedded DataProvider to load some data. The following logic is barely what happens in `Model::batch_predict()` method. See also `Model::fit()` method for the training data preparation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6f4f8-deb2-43c3-89b6-e0fcf2c97356",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=coach.trained['datarisk_cartoes']\n",
    "\n",
    "# Following line is here just to force use of cached parquet, if available\n",
    "model.context='batch_predict'\n",
    "\n",
    "# Get DP’s batch predict SQL queriesp\n",
    "dict_with_queries     = model.dp.get_dataset_sources_for_batch_predict()\n",
    "\n",
    "# Use queries to get multiple DataFrames\n",
    "dict_with_dataframes  = model.data_sources_to_data(dict_with_queries)\n",
    "\n",
    "# Integrate into one DataFrame and apply logic to clean data\n",
    "df                    = model.dp.clean_data_for_batch_predict(dict_with_dataframes)\n",
    "\n",
    "# Feature engineering\n",
    "df                    = model.dp.feature_engineering_for_batch_predict(df)\n",
    "\n",
    "# Resulting DataFrame used for batch predict\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6fd94f-43aa-434a-b786-fd45b45f2378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_with_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e743b",
   "metadata": {},
   "source": [
    "Compute estimations, finaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrative only. For you to see what pred_quantiles() does internally\n",
    "X_features=model.dp.get_features_list()\n",
    "\n",
    "# Don't need to filter by X_features, it will be filtered internally\n",
    "Y_pred=model.predict_proba(df)\n",
    "\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7e4ee",
   "metadata": {},
   "source": [
    "### Compute metrics\n",
    "\n",
    "Put data in right places so we can use convenient internal methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862cf321",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.batch_predict_data=df\n",
    "model.batch_predict_valuations=Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7db53a",
   "metadata": {},
   "source": [
    "Compute all metrics available for model, including methods provided by its DataProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98856d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_model_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b2c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_estimation_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8fc5ed",
   "metadata": {},
   "source": [
    "If `model.sets['train']`, `model.sets['val']` and `model.sets['test']` are defined and have data, this should work too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acab05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_trainsets_model_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af96f4f0",
   "metadata": {},
   "source": [
    "**POC 1** and **POC 2** unveil what happens in the [train workflow](https://github.com/loft-br/robson_avm/blob/main/.github/workflows/build_and_train_staging.yml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e958a-c01b-4023-89d2-7f6119cce2cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb4502-cffe-41c0-bdd7-3ed492ecbf76",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b647770f",
   "metadata": {},
   "source": [
    "## POC 3. Assess Metrics and create Comparative Reports\n",
    "Since all metrics are stored in DB, they can be assessed and compared.\n",
    "The `RobsonCoach` class has reporting tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa78ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics from staging and development DB\n",
    "\n",
    "os.environ.update(\n",
    "    dict(\n",
    "        XINGU_DB_URL=config_my_xingu.bundles['env']['beta_explorer']['XINGU_DB_URL']\n",
    "    )\n",
    ")\n",
    "\n",
    "coach=Coach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c2572c",
   "metadata": {},
   "source": [
    "Retrieve all metrics and metadata about 4 specific `train_id`s and show it in a comparative way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd96f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "report=coach.report(train_ids=['salmon-participant','wise-jacquard'])\n",
    "\n",
    "display(report['meta'])\n",
    "display(report['metrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2dc52c",
   "metadata": {},
   "source": [
    "### Display a subset of metrics: only the m² values for São Paulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a71fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "report['metrics'][['value per m²:São Paulo' in s for s in report['metrics'].index]].xs('global', level='set', axis=1).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae1466",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Display a subset of metrics: only the ones related to the `test` split part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8008a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "report['metrics'].xs('test', level='set', axis=1).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb61e861",
   "metadata": {},
   "source": [
    "### Save all metrics as Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011fb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel won't support time with timezone - how typical. Make it naïve.\n",
    "report['meta'].loc['time_utc']=report['meta'].loc['time_utc'].apply(lambda x: x.tz_convert(None))\n",
    "\n",
    "with pd.ExcelWriter(f'Metrics for Comitee Report — {pd.Timestamp.now().strftime(\"%Y.%m.%d-%H.%M.%S\")}.xlsx') as writer:\n",
    "\n",
    "    report_aux = report['meta'].sort_values(\"dataprovider_id\", axis=1)\n",
    "    report_aux.to_excel(writer, sheet_name=\"meta\")\n",
    "\n",
    "    dataprovider_list = list(set(report_aux.loc[\"dataprovider_id\", :]))\n",
    "\n",
    "    for dataprovider_id in dataprovider_list:\n",
    "\n",
    "        train_ids = list(report_aux.loc[:, report_aux.loc[\"dataprovider_id\", :] == dataprovider_id].columns)\n",
    "        train_session_ids = report_aux.loc[\"train_session_id\", report_aux.loc[\"dataprovider_id\", :] == dataprovider_id]\n",
    "\n",
    "        sheet = report[\"metrics\"].loc[:, report[\"metrics\"].columns.get_level_values(1).isin(train_ids)]\n",
    "\n",
    "        aux_list = {id: id + '|'+ train_session_ids[id] for id in sheet.columns.get_level_values(1)}\n",
    "\n",
    "        sheet = sheet.rename(columns=aux_list)\n",
    "\n",
    "        sheet.to_excel(writer, sheet_name=dataprovider_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e5289d",
   "metadata": {},
   "source": [
    "## POC 4. Check and report how Metrics evolved\n",
    "\n",
    "This example reports how metrics of same estimator evolved throughout time. We’ll use the production database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.update(config_my_xingu.bundles['env']['production'])\n",
    "\n",
    "coach=Coach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5667c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp='vitrine_sp'\n",
    "\n",
    "query=\"\"\"\n",
    "    select * from metrics_model\n",
    "    where dataprovider_id = '{dp}'\n",
    "    -- and set='global';\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad595a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from DB\n",
    "report=pandas.read_sql(query.format(dp=dp),con=coach.get_db_connection('xingu'))\n",
    "\n",
    "# Make time human readable\n",
    "report['time']=pd.to_datetime(report['time'], unit='s', utc=True)\n",
    "\n",
    "# Display a simple evolution report with just OKRs\n",
    "print(f\"Evolution of metrics for {dp}\")\n",
    "\n",
    "(\n",
    "    report[report['name'].str.contains('OKR')]\n",
    "    .set_index(['name','time'])\n",
    "    .drop(columns='dataprovider_id train_session_id train_id set value_text'.split())\n",
    "    .unstack()\n",
    "    .sort_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5251e4",
   "metadata": {},
   "source": [
    "How `OKR error > 15%:proportion` metric evolved through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac81d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "KPI=\"OKR error > 15%:proportion\"\n",
    "\n",
    "(\n",
    "    report\n",
    "    .query('name==@KPI')\n",
    "    [['time','value_number']]\n",
    "    .sort_values('time')\n",
    "    .set_index('time')\n",
    "    .plot\n",
    "    .line(title=f'{KPI} @ {dp}')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f2452c",
   "metadata": {},
   "source": [
    "## POC 5. Play with Cingu barebones\n",
    "\n",
    "`Coach` is handy to coordinate full trains, full batch predict process (including metrics computation) and multi-model loading. But you can play with `Model` objects directly too. A coach is still needed for DB access, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d3609",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_providers=['datarisk_cartoes']\n",
    "\n",
    "dpf=DataProviderFactory(providers_list=data_providers)\n",
    "\n",
    "coach=Coach(dpf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30519e67",
   "metadata": {},
   "source": [
    "Get an untrained object for `anuncios_scs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cd818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(\n",
    "    dp                     = next(coach.dp_factory.produce()),\n",
    "    coach                  = coach,\n",
    "    trained                = False,\n",
    "    delayed_prereq_binding = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c25a02e",
   "metadata": {},
   "source": [
    "Manualy load and bind pre-req models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60ed73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the coach to load them efficiently\n",
    "# coach.team_load(explicit_list=model.dp.pre_req)\n",
    "\n",
    "# Bind them to current model\n",
    "model.load_pre_req_model()\n",
    "\n",
    "# See result\n",
    "model.dp.pre_req_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1688273",
   "metadata": {},
   "source": [
    "Get DP’s SQL queries and related data, clean, integrate and engineer some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e587943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following line is here just to force use of cached parquet, if available\n",
    "model.context='train_dataprep'\n",
    "\n",
    "# Get DP’s batch predict SQL queries\n",
    "dict_with_queries     = model.dp.get_dataset_sources_for_train()\n",
    "\n",
    "# Use queries to get multiple DataFrames\n",
    "dict_with_dataframes  = model.data_sources_to_data(dict_with_queries)\n",
    "\n",
    "# Integrate into one DataFrame and apply logic to clean data\n",
    "df                    = model.dp.clean_data_for_train(dict_with_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7e71e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "df=model.dp.feature_engineering_for_train(df)\n",
    "\n",
    "# Resulting DataFrame used for batch predict\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1502b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dp.data_split_for_train(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f82f2b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## POC 6. Play with `ConfigManager`\n",
    "\n",
    "Reset this notebook before continuing. Run again only cells **before “POC 1”** just to setup environment.\n",
    "\n",
    "Here is `XINGU_DB_URL` env var with AWS secrets and parameters. Use `ConfigManager` to resolve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f39ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_my_robson.bundles['env']['beta_explorer']['XINGU_DB_URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78abbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.update(\n",
    "    dict(\n",
    "        ROBSON_DB_URL = config_my_robson.bundles['env']['beta_explorer']['XINGU_DB_URL']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e7b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfigManager.get('XINGU_DB_URL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21bf477",
   "metadata": {},
   "source": [
    "One more try. Reset its cache first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adcfc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfigManager.cache={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3500bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.update(\n",
    "    dict(\n",
    "        XYZ = '{%AWS_PARAM:robson-avm-staging-url%}/{%AWS_PARAM:robson-avm-staging-database-name%}'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfigManager.get('XYZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04505673",
   "metadata": {},
   "source": [
    "## POC 7. Xingu Estimators in the Command Line\n",
    "\n",
    "In staging and production environments, Xingu training is invoked in the command line. Inspect workflow files for [staging train](https://github.com/loft-br/robson_avm/blob/main/.github/workflows/build_and_train_staging.yml) and [staging hyper-param optimization](https://github.com/loft-br/robson_avm/blob/main/.github/workflows/build_and_hyperopt_staging.yml) to see how this is simply a mater of setting environment variables (as the ones on `config_my_xingu.py`) and running the `xingu` command as bellow. The `xingu` command source code is in [`/xingu/__main__.py`](https://github.com/loft-br/robson_avm/blob/main/robson/__main__.py).\n",
    "\n",
    "### All Xingu features can be controlled in the command line; see them all here\n",
    "\n",
    "```shell\n",
    "python3 -m xingu -h\n",
    "```\n",
    "\n",
    "### Train and Batch Predict 2 models in your laptop:\n",
    "\n",
    "This is fully parallel. One model will execute post-train actions (batch predict, data and pickle saving, metrics etc) while other model is being trained.\n",
    "\n",
    "```shell\n",
    "python3 -m xingu \\\n",
    "    --xingu-db \"sqlite:///xingu.db?check_same_thread=False\" \\\n",
    "    --datalake-athena \"awsathena+rest://athena.us-east-1.amazonaws.com:443/robson_valuation?work_group=mlops&compression=snappy\" \\\n",
    "    --datalake-databricks \"databricks+connector://token:dapi170fe70c366410b94bc76d2082ca01a3@dbc-da926df9-ab65.cloud.databricks.com/default?http_path=/sql/1.0/endpoints/b49aee71843b4d3e\" \\\n",
    "    --query-cache-path data \\\n",
    "    --trained-models-path models \\\n",
    "    --debug \\\n",
    "    --project-home . \\\n",
    "    --dps datarisk_cartoes,leves\n",
    "```\n",
    "\n",
    "### Batch Predict only in Production environment\n",
    "\n",
    "Note the `--no-train` parameter.\n",
    "\n",
    "```shell\n",
    "python3 -m xingu \\\n",
    "    --no-train \\\n",
    "    --robson-db \"postgresql+psycopg2://{%AWS_PARAM:robson-avm-production-user%}:{%AWS_SECRET:robson-avm-production-rds-secret%}@{%AWS_PARAM:robson-avm-production-url%}/{%AWS_PARAM:robson-avm-production-database-name%}\" \\\n",
    "    --datalake-athena \"awsathena+rest://athena.us-east-1.amazonaws.com:443/robson_valuation?work_group=mlops&compression=snappy\" \\\n",
    "    --datalake-databricks \"databricks+connector://token:dapi170fe70c366410b94bc76d2082ca01a3@dbc-da926df9-ab65.cloud.databricks.com/default?http_path=/sql/1.0/endpoints/b49aee71843b4d3e\" \\\n",
    "    --query-cache-path data \\\n",
    "    --trained-models-path \"s3://{%AWS_PARAM:xingu-production-bucket%}/trained-models\" \\\n",
    "    --debug \\\n",
    "    --project-home . \\\n",
    "    --dps anuncios_rj,anuncios_bh,anuncios_sp,cartorios,anuncios_gru,vitrine\n",
    "```\n",
    "\n",
    "### Hyper-parameters optimization only\n",
    "\n",
    "Notice how everything is turned off and disabled most parallelism to let Ray/SKOpt/Optimizer consume all CPUs\n",
    "\n",
    "```shell\n",
    "python3 -m xingu \\\n",
    "    --robson-db \"sqlite:///xingu.db?check_same_thread=False\" \\\n",
    "    --datalake-athena \"awsathena+rest://athena.us-east-1.amazonaws.com:443/robson_valuation?work_group=mlops&compression=snappy\" \\\n",
    "    --datalake-databricks \"databricks+connector://token:dapi170fe70c366410b94bc76d2082ca01a3@dbc-da926df9-ab65.cloud.databricks.com/default?http_path=/sql/1.0/endpoints/b49aee71843b4d3e\" \\\n",
    "    --query-cache-path data \\\n",
    "    --trained-models-path models \\\n",
    "    --debug \\\n",
    "    --project-home . \\\n",
    "    --no-post-process \\\n",
    "    --no-batch-predict \\\n",
    "    --hyperopt-strategy self \\\n",
    "    --parallel-train-max-workers 1 \\\n",
    "    --dps cartorios,anuncios_scs,listings\n",
    "```\n",
    "\n",
    "### Control Parallelism\n",
    "\n",
    "Explore these options to avoid over-subscribing and over-loading your CPU and RAM.\n",
    "\n",
    "```shell\n",
    "python3 -m xingu \\\n",
    "    --xingu-db \"sqlite:///xingu.db?check_same_thread=False\" \\\n",
    "    --datalake-athena \"awsathena+rest://athena.us-east-1.amazonaws.com:443/robson_valuation?work_group=mlops&compression=snappy\" \\\n",
    "    --datalake-databricks \"databricks+connector://token:dapi170fe70c366410b94bc76d2082ca01a3@dbc-da926df9-ab65.cloud.databricks.com/default?http_path=/sql/1.0/endpoints/b49aee71843b4d3e\" \\\n",
    "    --query-cache-path data \\\n",
    "    --trained-models-path models \\\n",
    "    --debug \\\n",
    "    --project-home . \\\n",
    "    --hyperopt-strategy self \\\n",
    "    --parallel-train-max-workers 3 \\\n",
    "    --parallel-hyperopt-max-workers 6 \\\n",
    "    --parallel-post-process-max-workers 3 \\\n",
    "    --parallel-estimators-max-workers 3\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a946ab",
   "metadata": {},
   "source": [
    "## POC 8. Deploy Robson Data and Estimators between environments\n",
    "### Staging to Production\n",
    "This is exactly what the [Deploy ⛔Production from ✅Staging GitHub Action](https://github.com/loft-br/robson_avm/actions/workflows/deploy_production_from_staging.yml) does.\n",
    "\n",
    "```shell\n",
    "python3 -m xingu.deploy \\\n",
    "    --source-xingu-db \"postgresql+psycopg2://{%AWS_PARAM:robson-avm-staging-user%}:{%AWS_SECRET:robson-avm-staging-rds-secret%}@{%AWS_PARAM:robson-avm-staging-url%}/{%AWS_PARAM:robson-avm-staging-database-name%}\" \\\n",
    "    --target-xingu-db \"postgresql+psycopg2://{%AWS_PARAM:robson-avm-production-user%}:{%AWS_SECRET:robson-avm-production-rds-secret%}@{%AWS_PARAM:robson-avm-production-url%}/{%AWS_PARAM:robson-avm-production-database-name%}\" \\\n",
    "    --source-trained-models-path \"s3://{%AWS_PARAM:robson-avm-staging-bucket%}/trained-models\" \\\n",
    "    --target-trained-models-path \"s3://{%AWS_PARAM:robson-avm-production-bucket%}/trained-models\" \\\n",
    "    --project-home . \\\n",
    "    --debug\n",
    "```\n",
    "### Build API Container with Production Estimators\n",
    "Note how `--dps` is not being used, causing it to act on all DataProviders. Note the `--no-db` parameter, to not copy DB entries, because the production API doesn’t use the Xingu database.\n",
    "```shell\n",
    "git clone git@github.com:avibrazil/xingu.git;\n",
    "cd xingu;\n",
    "# Change to production branch\n",
    "git checkout deploy-command;\n",
    "\n",
    "python3 -m xingu.deploy \\\n",
    "    --source-xingu-db \"postgresql+psycopg2://{%AWS_PARAM:robson-avm-production-user%}:{%AWS_SECRET:robson-avm-production-rds-secret%}@{%AWS_PARAM:robson-avm-production-url%}/{%AWS_PARAM:robson-avm-production-database-name%}\" \\\n",
    "    --source-trained-models-path \"s3://{%AWS_PARAM:robson-avm-production-bucket%}/trained-models\" \\\n",
    "    --target-trained-models-path models \\\n",
    "    --project-home . \\\n",
    "    --no-db \\\n",
    "    --debug;\n",
    "```\n",
    "\n",
    "### Production to Laptop or SageMaker\n",
    "```shell\n",
    "git clone git@github.com:avibrazil/xingu.git;\n",
    "cd robson_avm;\n",
    "# Change to production branch\n",
    "git checkout deploy-command;\n",
    "\n",
    "python3 -m xingu.deploy \\\n",
    "    --dps anuncios_sp,vitrine_sp \\\n",
    "    --source-xingu-db \"postgresql+psycopg2://{%AWS_PARAM:robson-avm-production-user%}:{%AWS_SECRET:robson-avm-production-rds-secret%}@{%AWS_PARAM:robson-avm-production-url%}/{%AWS_PARAM:robson-avm-production-database-name%}\" \\\n",
    "    --target-xingu-db \"sqlite:///robson.db?check_same_thread=False\" \\\n",
    "    --source-trained-models-path \"s3://{%AWS_PARAM:robson-avm-production-bucket%}/trained-models\" \\\n",
    "    --target-trained-models-path models \\\n",
    "    --project-home . \\\n",
    "    --debug;\n",
    "```\n",
    "\n",
    "### Staging to Laptop or SageMaker\n",
    "Manually edit `inventory.yaml` to correctly map desired `train_ids` to `dataprovider_ids`, and then:\n",
    "```shell\n",
    "python3 -m xingu.deploy \\\n",
    "    --dps anuncios_sp,anuncios_rj,anuncios_scs,cartorios \\\n",
    "    --source-xingu-db \"postgresql+psycopg2://{%AWS_PARAM:robson-avm-staging-user%}:{%AWS_SECRET:robson-avm-staging-rds-secret%}@{%AWS_PARAM:robson-avm-staging-url%}/{%AWS_PARAM:robson-avm-staging-database-name%}\" \\\n",
    "    --target-xingu-db \"sqlite:///robson.db?check_same_thread=False\" \\\n",
    "    --source-trained-models-path \"s3://{%AWS_PARAM:robson-avm-staging-bucket%}/trained-models\" \\\n",
    "    --target-trained-models-path models \\\n",
    "    --project-home . \\\n",
    "    --debug;\n",
    "```\n",
    "\n",
    "### Laptop or SageMaker to Staging (go to committee)\n",
    "Your `inventory.yaml` has the `train_id` of an estimator that you just trained for a certain `dataprovider_ids`.\n",
    "```shell\n",
    "python3 -m xingu.deploy \\\n",
    "    --dps vitrine \\\n",
    "    --source-xingu-db \"sqlite:///robson.db?check_same_thread=False\" \\\n",
    "    --target-xingu-db \"postgresql+psycopg2://{%AWS_PARAM:robson-avm-staging-user%}:{%AWS_SECRET:robson-avm-staging-rds-secret%}@{%AWS_PARAM:robson-avm-staging-url%}/{%AWS_PARAM:robson-avm-staging-database-name%}\" \\\n",
    "    --source-trained-models-path models \\\n",
    "    --target-trained-models-path \"s3://{%AWS_PARAM:robson-avm-staging-bucket%}/trained-models\" \\\n",
    "    --project-home . \\\n",
    "    --debug;\n",
    "```\n",
    "\n",
    "### Partial deployment or deployment failed?\n",
    "Low RAM can hurt data extraction bacause `SELECT`s might return several million lines of data. Deploy command tries to transfer data in chunks of variable size, based on the detected RAM. If it fails, use the `--db-page-size` parameter with values as low as 200000. It will take longer but it won’t fail.\n",
    "\n",
    "```shell\n",
    "python3 -m xingu.deploy \\\n",
    "    ...\n",
    "    --db-page-size 200000 \\\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca99fe5",
   "metadata": {},
   "source": [
    "## POC 9. Explain Estimations with Shapley Values\n",
    "Shapley value is a number received by each feature used as input to an estimation. It has 2 dimensions:\n",
    "\n",
    "1. Strength, which tells how much this feature influenced the estimation\n",
    "2. Signal, which tells if the feature influeced the estimation to go above (+) or below (-) the model average\n",
    "\n",
    "The SHAP module extracts information from a trained model, computes Shapley values per feature per estimation and is capable of producing high impact graphics to help explain the forces that influenced that estimation or the model modus operandi.\n",
    "\n",
    "To start, import high performance modules capable of using multiple cores (original shap module is inneficient in this regard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc11c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttreeshap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd7673",
   "metadata": {},
   "source": [
    "Load estimator and its pre-reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68443769",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp='vitrine_sp'\n",
    "\n",
    "data_providers=[dp]\n",
    "dpf=DataProviderFactory(providers_list=data_providers)\n",
    "coach=Coach(dpf)\n",
    "coach.team_load()\n",
    "model=coach.trained[dp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05871c6d",
   "metadata": {},
   "source": [
    "Get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "city='São Paulo'\n",
    "\n",
    "query=f'select * from table where city={city}'\n",
    "\n",
    "df=pd.read_sql_query(query,con=coach.get_db_connection('datalake_athena'))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038f69a7",
   "metadata": {},
   "source": [
    "Simple clean data, compute features with pre-req model and estimate prices for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd8036",
   "metadata": {},
   "outputs": [],
   "source": [
    "org=df\n",
    "# df=org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c4104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=(\n",
    "    # Estimate with pre-req model\n",
    "    model.dp.feature_engineering_for_predict(\n",
    "        # clean it first\n",
    "        df.dropna(\n",
    "            subset=model.dp.get_features_list()\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Estimate prices and errors\n",
    "df=(\n",
    "    pandas.concat([df,model.predict(df)], axis=1)\n",
    "    .assign(\n",
    "        estimation  =lambda table: numpy.exp(table['loc']),\n",
    "        error_abs_1p=lambda table: table.estimation-table.last_transaction_value_1p_per_meter,\n",
    "        error_abs_3p=lambda table: table.estimation-table.last_transaction_value_3p_per_meter,\n",
    "        error_pct_1p=lambda table: (table.estimation-table.last_transaction_value_1p_per_meter)/table.last_transaction_value_1p_per_meter,\n",
    "        error_pct_3p=lambda table: (table.estimation-table.last_transaction_value_3p_per_meter)/table.last_transaction_value_3p_per_meter\n",
    "    )\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92298ad8",
   "metadata": {},
   "source": [
    "Distribution of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be664bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .dropna(subset=['last_transaction_value_1p_per_meter'])\n",
    "    .query(\"last_transaction_value_1p!=0\")\n",
    "    .error_pct_1p\n",
    "    .plot\n",
    "    .hist(bins=80,figsize=(20, 6))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69669680",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .dropna(subset=['last_transaction_value_3p_per_meter'])\n",
    "    .query(\"last_transaction_value_3p!=0\")\n",
    "    .error_pct_3p\n",
    "    .plot\n",
    "    .hist(bins=200, figsize=(20, 6), range=(-1,1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafafd49",
   "metadata": {},
   "source": [
    "### Compute and visualize Shapley values\n",
    "Get a fast Shapley explainer. The `model_output=0` flags NGBoost to work with `loc`. If `=1`, works with scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94959d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer = fasttreeshap.TreeExplainer(\n",
    "    model                   = model.estimator.bagging_members[0],\n",
    "#     data                    = df[model.dp.get_estimator_features_list()],\n",
    "#     feature_perturbation    =\"interventional\",\n",
    "    model_output    = 0\n",
    ")\n",
    "\n",
    "# Expected value is usually close to the mean for an Y_pred computed on train process\n",
    "shap_explainer.expected_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e88fb68",
   "metadata": {},
   "source": [
    "Compute Shapley values for a few `unit_id`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a76019",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_id=[133759,5850832]\n",
    "df[df.unit_id.isin(unit_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e31b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.unit_id.isin(unit_id)][model.dp.get_estimator_features_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2da1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values=shap_explainer.shap_values(df[df.unit_id.isin(unit_id)][model.dp.get_estimator_features_list()])\n",
    "shap_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138231c1",
   "metadata": {},
   "source": [
    "Same thing, but now inspect values in a more semantic way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e35ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=pd.DataFrame(\n",
    "    data     = shap_explainer.shap_values(df[df.unit_id.isin(unit_id)][model.dp.get_estimator_features_list()]),\n",
    "    columns  = model.dp.get_estimator_features_list(),\n",
    "    index    = df[df.unit_id.isin(unit_id)].unit_id\n",
    ")\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26159cd9",
   "metadata": {},
   "source": [
    "### Visualize Explanations for 1 Datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttreeshap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9dd986",
   "metadata": {},
   "source": [
    "* Red features push the target above the base value\n",
    "* Blue features push target below base value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttreeshap.force_plot(\n",
    "    base_value     = shap_explainer.expected_value,\n",
    "    shap_values    = s.iloc[[1]].to_numpy(),\n",
    "    features       = df[df.unit_id==s.iloc[[1]].index[0]][model.dp.get_estimator_features_list()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f30e744",
   "metadata": {},
   "source": [
    "### Same, but for pre req estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prereq=model.dp.pre_req_model['anuncios_sp']\n",
    "\n",
    "prereq_shap_explainer = fasttreeshap.TreeExplainer(\n",
    "    model=model_prereq.estimator.bagging_members[0],\n",
    "    model_output=0\n",
    ")\n",
    "\n",
    "s=pd.DataFrame(\n",
    "    data=prereq_shap_explainer.shap_values(df[df.unit_id.isin(unit_id)][model_prereq.dp.get_estimator_features_list()]),\n",
    "    columns=model_prereq.dp.get_estimator_features_list(),\n",
    "    index=df[df.unit_id.isin(unit_id)].unit_id\n",
    ")\n",
    "\n",
    "fasttreeshap.force_plot(\n",
    "    base_value=prereq_shap_explainer.expected_value,\n",
    "    shap_values=s.iloc[[0]].to_numpy(),\n",
    "    features=df[df.unit_id==s.iloc[[0]].index[0]][model_prereq.dp.get_estimator_features_list()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a55176b",
   "metadata": {},
   "source": [
    "### Visualize Explanations for Many Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e850deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "size=2000\n",
    "seed=40\n",
    "\n",
    "s=pd.DataFrame(\n",
    "    data         = shap_explainer.shap_values(df.sample(size, random_state=seed)[model.dp.get_estimator_features_list()]),\n",
    "    columns      = model.dp.get_estimator_features_list(),\n",
    "    index        = pd.Index(df.sample(size, random_state=seed).unit_id,name='unit_id')\n",
    ")\n",
    "\n",
    "fasttreeshap.force_plot(\n",
    "    base_value   = shap_explainer.expected_value,\n",
    "    shap_values  = s.to_numpy(),\n",
    "    features     = df.sample(size, random_state=seed)[model.dp.get_estimator_features_list()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0da879",
   "metadata": {},
   "source": [
    "### Visualize Global Model Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ccdb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "fasttreeshap.summary_plot(s.to_numpy(),s, plot_type='violin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4858561c",
   "metadata": {},
   "source": [
    "* Features/columns are ordered from more important, top do bottom\n",
    "* SHAP value indicates importance of the feature to force target up (+) or down (-)\n",
    "* Colors indicate value of feature, having blue as low value and red as high value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced560d",
   "metadata": {},
   "source": [
    "### Numerical Feature Importance from Shapley\n",
    "\n",
    "Feature importance for the sample dataset above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a88579",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pd.DataFrame(s, columns=model.dp.get_estimator_features_list())\n",
    "    .abs()\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3a2bec",
   "metadata": {},
   "source": [
    "### Analyze Estimations where Error is higher than 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c54162",
   "metadata": {},
   "outputs": [],
   "source": [
    "asample=(\n",
    "    df\n",
    "    .dropna(subset=['last_transaction_value_3p_per_meter'])\n",
    "    .query(\"error_pct_3p>=0.3\")\n",
    ")\n",
    "\n",
    "s=pd.DataFrame(\n",
    "    data         = shap_explainer.shap_values(asample[model.dp.get_estimator_features_list()]),\n",
    "    columns      = model.dp.get_estimator_features_list(),\n",
    "    index        = pd.Index(asample.unit_id,name='unit_id')\n",
    ")\n",
    "\n",
    "fasttreeshap.force_plot(\n",
    "    base_value   = shap_explainer.expected_value,\n",
    "    shap_values  = s.to_numpy(),\n",
    "    features     = asample[model.dp.get_estimator_features_list()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde0f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb51b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_unit(unit_id):\n",
    "    print(f'Shap base value: R${numpy.exp(shap_explainer.expected_value)[0]:.2f}')\n",
    "    print(f'Estimation:      R${df.query(\"unit_id == @unit_id\")[\"estimation\"].values[0]:.2f}')\n",
    "    print(f'Sold:            R${df.query(\"unit_id == @unit_id\")[\"last_transaction_value_3p_per_meter\"].values[0]:.2f}')\n",
    "    print(f'Error%:            {100*df.query(\"unit_id == @unit_id\")[\"error_pct_3p\"].values[0]:.2f}%')\n",
    "\n",
    "    return fasttreeshap.force_plot(\n",
    "        base_value=shap_explainer.expected_value,\n",
    "        shap_values=s.loc[unit_id].to_numpy(),\n",
    "        features=df[df.unit_id==s.loc[[unit_id]].index[0]][model.dp.get_estimator_features_list()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1743d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_id=2886639\n",
    "\n",
    "explain_unit(unit_id)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
